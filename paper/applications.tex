\subsection{Sorting}
\label{subsec:sorting}

        To demonstrate the efficiency of our algorithm, we applied it to a popular computational task that demands multiple comparisons, sorting.
	The best homomorphic sorting algorithm in terms of running time is the direct sorting algorithm due to~\cite{CDSS15}.
	For a given array $\va = (a_0,\dots,a_{n-1})$, this algorithm computes a \emph{comparison matrix} $\mL$ whose entries are defined by
	\begin{align*}
		\mL_{ij} =
		\begin{cases}
			\LT(a_i, a_j) & \text{ if } i < j, \\
			0 & \text{ if } i = j, \\
			1 - \LT(a_j, a_i) & \text{ if } i > j.
		\end{cases}
	\end{align*}
	It is easy to see that the Hamming weight of the $i$th row of $\mL$ is unique and equal to the array index of $a_i$ after sorting $\va$ in the descending order.
	For example, the zero weight indicates that there are no elements of $\va$ bigger than $a_i$; thus, $a_i$ is the first value of $\va$ after sorting, or the maximum element of $\va$.

	To arrange $\va$ in the right order, we extract its elements by comparing the current array index $i$ with the Hamming weights of the comparison matrix rows ($\wt(\mL[j])$).  
	\begin{align*}
		a'_i = \sum_{j=0}^{n-1} \EQ(i,\wt\left(\mL[j]\right)) \cdot a_j.
	\end{align*}
	Since each row has a unique Hamming weight, there exist only one $a_j$ that will be assigned to $a'_i$.

	\begin{remark}
		Since the matrix $\mL$ is defined by $n(n-1)/2$ elements, it can be costly to keep it in memory.
		Instead we can compute the Hamming weights of its rows by iteratively computing one comparison $\LT(a_i, a_j)$ with $i < j$ at a time.
		To achieve this, we create an array of size $n$ initialized with zeros that eventually will store the Hamming weights.
		Then, we add the outcome of $\LT(a_i, a_j)$ to the $i$th element of this array and the result of $1-\LT(a_i, a_j)$ to the $j$th element.
		In this approach, only $n$ elements of the Hamming weight array are being kept in RAM.
	\end{remark}

	The direct sorting algorithm requires $n(n-1)/2$ less-than operations and $n^2$ equality operations.
	While computing equalities, we can reduce the total number of ciphertext-ciphertext multiplications if $n$ is large enough.
	Recall that $\EQ(i,\wt\left(\mL[j]\right)) = 1 - (i-\wt\left(\mL[j]\right))^{p-1}$, which implies that $\EQ$ needs $M = \log_2(p-1) + \wt(p-1) - 1$ ciphertext-ciphertext multiplications.
	Hence, to compute $\EQ(i,\wt\left(\mL[j]\right))$ for all $i \in [0,n-1]$, we should perform $n M$ multiplications.
	Using Lemma~\ref{lem:difference_to_p-1}, we can rewrite $\EQ(x,y) = 1 - \sum_{k=0}^{p-1} i^k \cdot \wt\left(\mL[j]\right)^{p-1-k}$.
	If we precompute the powers $\wt\left(\mL[j]\right)^{p-1-k}$, then we need only $p-2$ multiplications to compute all the equalities $\EQ(i,\wt\left(\mL[j]\right))$.
	Hence, if $p-2 < nM$, or $n > (p-2)/M$, the latter approach results in a smaller number of ciphertext-ciphertext multiplications.
	One can argue that the latter approach introduces $p-1$ plaintext-ciphertext multiplications ($i^k \cdot \wt\left(\mL[j]\right)^{p-1-k}$) and $p-2$ additions. 
	However, these operations are much faster in practice than ciphertext-ciphertext multiplication such that the gain from reduced ciphertext-ciphertext multiplications becomes dominant. 
	
	The main advantage of direct sorting is that its multiplicative depth is independent of the array length, namely $d = d(\LT) + d(\EQ) + d(\wt) + 1$.
	This allows to avoid large encryption parameters and costly bootstrapping operations.
	We can further reduce this depth by computing the Hamming weight modulo a plaintext modulus $p$ that is equal or larger than the length of an array $n$.

\subsection{Min/max of an array}
\label{sec:min/max}

	Another application of our comparison algorithm is concerned with the minimum (or maximum) function of an array.
	To find the minimum of an input array, at least $n-1$ calls of the pairwise minimum function are required~\cite[Chapter 9]{CLR09}, which can be achieved by \emph{the tournament method}.
	In this method, one reduces the size of the input array in $\ceil{\log n}$ iterations.
	In each iteration, the input array is divided into pairs. 
	If the array length is odd, one element is stashed for the next iteration. 
	Then, the maximum of each pair is removed from the array.
	The algorithm stops when only one element is left; this is the minimum of the input array, see Figure~\ref{fig:minimum_tournament}. 
	\begin{figure}
		\centering
		\begin{tikzpicture}
			% first stage
			\draw[black, thin] (0,0) rectangle (1,1);
			\draw[black, thin] (0,1.5) rectangle (1,2.5);
			\draw[black, thin] (0,3.0) rectangle (1,4);
			\draw[black, thin] (0,4.5) rectangle (1,5.5);

			\node at (0.5, 5.0) {$a_0$};
			\node at (0.5, 3.5) {$a_1$};
			\node at (0.5, 2.0) {$a_2$};
			\node at (0.5, 0.5) {$a_3$};

			% second stage
			\draw[black, thin] (1.5,0.75) rectangle (2.5,1.75);
			\draw[black, thin] (1.5,3.75) rectangle (2.5,4.75);

			\node at (2.0, 1.25) {$\min$};
			\node at (2.0, 4.25) {$\min$};

			\draw[black, thin, ->] (1,0.5) -- (2,0.5) -- (2,0.75);
			\draw[black, thin, ->] (1,2) -- (2,2) -- (2,1.75);

			\draw[black, thin, ->] (1,3.5) -- (2,3.5) -- (2,3.75);
			\draw[black, thin, ->] (1,5) -- (2,5) -- (2,4.75);

			% third stage
			\draw[black, thin] (3.0,2.25) rectangle (4.0,3.25);

			\node at (3.5, 2.75) {$\min$};

			\draw[black, thin, ->] (2.5,1.25) -- (3.5,1.25) -- (3.5,2.25);
			\draw[black, thin, ->] (2.5,4.25) -- (3.5,4.25) -- (3.5,3.25);

			% final outcome
			\draw[black, thin, ->] (4.0,2.75) -- (5.0,2.75);
			\node at (6.5, 2.75) {$\min(a_0,a_1,a_2,a_3)$};

		\end{tikzpicture}
		\caption{The tournament method of finding the minimum of an array. In each stage, the array elements are divided into pairs. Only minimum of a pair go to the next stage.}
		\label{fig:minimum_tournament}
	\end{figure}
	Unfortunately, the tournament method has a big multiplicative complexity, namely $\ceil{\log n} \cdot d(\min(x,y))$.
	In the HE world, this enforces us to use either impractical encryption parameters or a slow bootstrapping function.

	To reduce the depth, we can combine the tournament method and direct sorting.
	First, we perform $\ell$ iterations of the tournament algorithm, which leaves us with an array $\va' = (a'_0, \dots, a'_{n'-1})$ of length $n' = \ceil{n/2^\ell}$ containing the minimum.
	Then, we can find the minimum by computing the comparison table as in direct sorting and extracting the minimal element.
	This can be done with two methods.

	First, one can leverage the fact that the Hamming weight of the comparison table row corresponding to the minimal element is equal to $n'-1$.
	Hence, we can retrieve the minimal element as follows
	\begin{align}\label{eq:our_minimum}
		\min(\va') = \sum_{i=0}^{n'-1} \EQ(n'-1,\wt\left(\mL[i]\right)) \cdot a'_i.
	\end{align}
	Here, the multiplicative depth is reduced to 
	\begin{align*}
		\ell \cdot d(\min(x,y)) + d(\LT(x,y)) + d(\EQ(x,y)) + 1 =
		(\ell + 2) \cdot \ceil{\log (p-1)} + 1
	\end{align*}
	which is independent of the array length $n$.
	If $M(\EQ(x,y))$ and $M(\LT(x,y))$ is the number of multiplications in the equality function $\EQ$ and the less-than function $\LT$, respectively, the multiplicative complexity is equal to 
	\begin{align*}
		n (1 + M(\EQ(x,y))) + \frac{n(n-1)}{2} \cdot  M(\LT(x,y)).
	\end{align*}
	Shaul et al.~\cite{PoPETS:SFR20} proposed another circuit for the minimum function that exploits the fact that the comparison table row related to the minimal element contains only $1$ except for the main diagonal element.
	In other words, the product of $\prod_{j=1, j \ne i}^{n} \mL_{ij} = 1$ if and only if $a'_i = \min(\va')$.
	Hence, the minimal element is equal to
	\begin{align}\label{eq:shaul_minimum}
		\min(\va') = \sum_{i=0}^{n'-1} a'_i \cdot \prod_{j=1, j \ne i}^{n} \mL_{ij}.
	\end{align}
	The resulting depth of this circuit amounts to
	\begin{align*}
		\ell \cdot d(\min(x,y)) + d(\LT(x,y)) + \ceil{\log (n-1)} + 1 \\
		= (\ell + 1) \cdot \ceil{\log (p-1)} + \ceil{\log (n-1)} + 1. 
	\end{align*}
	This circuit requires the following number of multiplications 
	\begin{align*}
		n (n-1) + \frac{n(n-1)}{2} \cdot M(\LT(x,y)) .
	\end{align*}
	This implies that for small enough $n$, Shaul's circuit~(\ref{eq:shaul_minimum}) has smaller depth or smaller multiplication complexity than the circuit in~(\ref{eq:our_minimum}).
	Furthermore, Shaul's circuit is correct even if $n > p$, whereas (\ref{eq:our_minimum}) requires $n \le p$ such that $\wt(\mL[i])$ are computed correctly.

	In the experiments conducted in Section~\ref{sec:impl-results}, we use the best of these approaches for given $n$ and $p$. 
	%%%%%%%%%%
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
